# CSV Race

Welcome to CSV Race repository!

I needed a repository to benchmark my CSV parser and fine tune it and also to monitor the behavior
of different parser with different outputs. I decided to prepare this repository to test CSV parsers from different
languages and compare them with a simple test case on various different types of files and see the results.

I would like this repository to evolve with all the new libraries that I'm sure will emerge and improve upon existing
parsers. The goal is to provide an easy way to benchmark for CSV parser authors or folks who want to find the
best parser for their task at hand. While I go over some of the benchmark data in the readme file. The repository
provides scripts and means for you to generate or use your own test data and your own set of libraries and generate
your own result. Refer to [Running the benchmarks](#running-the-benchmarks) to learn how to do this.

I have used this extensively while working on [csv-zero](https://github.com/peymanmortazavi/csv-zero) and it has proved helpful.

> [!HINT] It may be boring to read all the ways this benchmark is done but I invite you to read the [Benchmark
> Methodology](#benchmark-methodology) section before you look at the data.

## Benchmark Methodology

There are a lot of benchmark data out there and a lot of them are not done with the level of care necessary.
I would like to make it clear that despite the fact that I have tried to test these libraries with various different
file sizes and styles, which will be discussed shortly, you should not make decisions entirely by looking at these
charts and take further look into the libraries. They may handle your specific cases better, they may offer some
features or extra utilities that are simlpy not available in other libraries. Even if your goal is entirely
performance, you should examine the data and make sure the benchmark data reflects your specific needs, matches your
execution environment, etc.

> [!INFO] I have tried my best to choose various test cases, find best libraries out there and use them as they should
> be used. If you spot that I have made a mistake, please feel free to offer corrections or suggest a missing library.

### Task

This benchmark wants to focus on speed of iteration and not what consumers might want to do with that data. For this
reason, each parser must iterate and count the number of fields in a CSV file as fast as it can using only `64 KB`
buffer size. Some libraries might not provide a way to specify this, for those, we have not enforced a buffer size.

### Test Files

While searching for CSV files used for benchmarking, I came across some common files and decided that we should include
them. They are good general files to test files with a good mixture of quoted and non-quoted regions, with escapes and
different sizes. These files are perhaps closer to what a real-world csv file would be but if you are working with
larger data or delve into more specific cases, you might want to generate or use your own data and run the benchmark to
visualize benchmark data.

The four files are `game.csv`, `gtfs-mbta-stop-times.csv`, `nfl.csv` and `worldcitiespop.csv`.

There are also some files that are generated to test some other cases and their name follows this format:

`<size>_<mix|no>_quotes_<column-count>_col_<min-field-size>_<max-field-size>.csv`

Where `size` indicates the file size, `col` indicates how many columns are used.

The content of each field (cell) is generated by first randomly choosing a `len` in range (`min-field-size`,
`max-field-size`) and then randomly picking printable ASCII characters from range `32` to `127`.

If quotations are disabled (`no-quotes`), `"`, `\` and `,` characters never get included. Otherwise they can and if
they do, that field would get wrapped in `"` as a correct CSV file should.

- `xs_mix_quotes_12_col_0_32.csv`: X-Small (~1 KB), Mix of quoted fields, 12 columns, 0-32 chars in each field
- `xs_no_quotes_52_col_0_256.csv`: X-Small (~330 KB), No quoted fields, 52 columns, 0-256 chars in each field
- `m_mix_quotes_12_col_0_32.csv`: Medium (~102 M), Mix of quoted fields, 12 columns, 0-32 chars in each field
- `m_no_quotes_52_col_0_256.csv`: Medium (~32 M), No quoted fields, 52 columns, 0-256 chars in each field
- `xl_mix_quotes_2_col_0_12_many_rows.csv`: X-Large (~700 M) Mix of quoted fields, 2 columns, 0-12 chars in each field
- `xl_no_quotes_52_col_0_256.csv`: X-Large (~3.2 G) No quoted fields, 52 columns, 0-256 chars in each field
- `xl_mix_quotes_12_col_0_32.csv`: X-Large (~9.9 G) Mix of quoted fields, 12 columns, 0-32 chars in each field

### Data Points

The time it takes to complete the task is very important but we try to capture other important metrics as well.

- `Wall Time`: Wall time is the time it takes to really complete the task.
- `Peak RSS`: The peak resident set size (RSS) is the maximum portion of memory occupied by the parser process.
- `CPU Instructions`: Total number of machine-level instructions retired by the parser process during execution.
  This reflects the amount of work performed by the CPU, independent of clock speed.
- `CPU Cycles`: Total number of CPU clock cycles elapsed while executing the parser process.
  This includes cycles spent executing instructions as well as cycles stalled waiting on memory, branch resolution,
  or other microarchitectural events.
- `Cache References`: A cache reference is counted when the CPU attempts to access data via the cache hierarchy. In
  other words, events like memory access that went through cache system.
- `Cache Misses`: All cache misses across all levels (mostly LLC misses).
- `Branch Misses`: All branch mispredictions. If you are not familiar with this, reading [Branch predictor](https://en.wikipedia.org/wiki/Branch_predictor) might prove helpful.

### Tooling

[Poop](https://github.com/andrewrk/poop) is the tooling used to produce these metrics, which in turn is provided by
`perf`, a tool and a library used to provide CPU and hardware performance metrics. Unfortunately, `poop` only supports
linux. If you are using mac or Windows, you can use [Hyperfine](https://github.com/sharkdp/hyperfine) but it will only
provide you with wall time metrics. You can use `perf` or alternative tools.

### Data Visualization

A simple Python script is used to run the tests using `poop` to provide a CSV file and some figures of the metrics
stated above. You can re-purpose the script to use your own tooling or to tweak the charts.

## Benchmark Data

> [!NOTE] The benchmark is measured using cpu `AMD Ryzen 5 PRO 5650U with Radeon Graphics`, `30 GB` memory on a linux
> operating system system `6.17.8-arch1-1`.

The result is indeed different for different computers and CPUs. I would love to provide visualizations for other CPU
architectures as well at some point, perhaps you can help with that!

> [!NOTE] In order to reduce the noise, only the top 5 parsers are chosen. To see the raw numbers for each parser,
> refer to result-all.csv which includes every test case and every parser. The figures only include the 4 common csv
> files for the most part and the top 5 parsers in terms of their performance.

Let's get to the exciting bits!

### Wall Time

So how long do different parsers take to handle the 4 common test cases?

![CSV Parser Wall Time Comparison](images/wall_time.png "CSV Parser Wall Time Comparison")

## Running the benchmarks

TODO: talk about generation, pre-requisites, poop vs crap.
